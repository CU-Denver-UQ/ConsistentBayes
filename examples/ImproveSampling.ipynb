{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent Bayes: Improving Sampling\n",
    "---\n",
    "\n",
    "Copyright 2018 Michael Pilosov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "_tested with python 3.6 on 01/26/18_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematics and Plotting\n",
    "from HelperFuns import * # pyplot wrapper functions useful for visualizations, numpy, scipy, etc.\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.rcParams['figure.figsize'] = 10, 5\n",
    "from cbayes import sample, solve, distributions\n",
    "# Interactivity\n",
    "from ipywidgets import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Formulating the Inverse Problem\n",
    "---\n",
    "### Prior Information/Assumptions\n",
    "\n",
    "* We assume that the true value $\\lambda_0$ belongs to a parameter space $\\Lambda$.\n",
    "\n",
    "\n",
    "* Much like in the classical statistical Bayesian framework, we begin by encapsulating our pre-existing beliefs about our parameters in a distribution in a prior distribution on $\\Lambda$, $\\pi^{prior}_\\Lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Observed Density\n",
    "\n",
    "* The observed density represents the uncertainty in an observation of a measurable quantity of interest map that takes input parameters and produces a vector in $\\mathbb{R}^d$.\n",
    "\n",
    "\n",
    "* While there are problem scenarios you can posit where the observed density corresponds to a normalized likelihood function from the statistical Bayesian approach, the quantity of interest may not necessarily just be the uncertainty in the measurement data. \n",
    "\n",
    "\n",
    "* If the quantity of interest is indeed a single direct measurement, then the likelihood is the observed. For example, for some true parameter $\\lambda_0$ and model $u(\\lambda, t)$, suppose your quantity of interest is defined as a single evaluation at some time $t_0$. The measurement uncertainty contained in that one measurement would constitute your observed density. \n",
    "\n",
    "\n",
    "* However, if we have a collection of observations, such as at $t_0, t_2, \\dots, t_K$,  each of which is polluted by normally distributed noise, a common thing to do from Bayesian and Frequentist statistics would be to minimize the mean-or sum-squared error. If the sum squared error (SSE) is what we treat as our quantity of interest, the observed density on $\\mathcal{D}$, denoted by $\\pi^{obs}_{\\mathcal{D}}(d)$, would be given by a $\\chi^2_{K+1}$ distribution.\n",
    "\n",
    "\n",
    "### The Posterior Density\n",
    "\n",
    "* Let $\\pi^{O(prior)}_{\\mathcal{D}}(d)$ denote the push-forward of the prior density onto $\\mathcal{D}$. Then, the posterior density on $\\Lambda$ is given by\n",
    "\n",
    "$$\n",
    "    \\pi^{post}_\\Lambda(\\lambda) := \\pi^{prior}_\\Lambda(\\lambda)\\frac{\\pi^{obs}_{\\mathcal{D}}(Q(\\lambda))}{\\pi^{O(prior)}_{\\mathcal{D}}(Q(\\lambda))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define your Map\n",
    "_ Choose from one of the following example options, feel free to add your own _ \n",
    "\n",
    "$O_1(\\lambda) = \\sum_{i=1}^n \\lambda_i$  \n",
    "\n",
    "$O_2(\\lambda) = \\lbrace \\lambda_0,\\;\\; \\lambda_0 - \\lambda_1 \\rbrace$ \n",
    "\n",
    "$O_3(\\lambda) = \\lbrace \\lambda_0,\\;\\; \\lambda_0 - \\lambda_1, \\; \\;\\lambda_1^2 \\rbrace$\n",
    "\n",
    "$O_4(\\lambda) = (1-x)^2 + (y - x^2)^2$  ( This is the [Rosenbrock Function](https://en.wikipedia.org/wiki/Rosenbrock_function) with $a=1$ and $b=100$. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PtO_fun_choice = 4\n",
    "\n",
    "def fun1(lam): # sum all params\n",
    "    return np.sum(lam,axis=1)\n",
    "\n",
    "def fun2(lam): # pull two params, linear combination.\n",
    "    return np.array([ lam[:,0] ,lam[:,0]-lam[:,1]]).transpose()\n",
    "\n",
    "def fun3(lam): # pull two params, linear combination.\n",
    "    return np.array([ lam[:,0] ,lam[:,0]-lam[:,1], lam[:,1]**2 ]).transpose()\n",
    "\n",
    "def rosenbrock(lam):\n",
    "     return (1.-lam[:,0])**2 + 100*(lam[:,1]-lam[:,0]**2.)**2\n",
    "    \n",
    "if PtO_fun_choice == 1:\n",
    "    PtO_fun = fun1\n",
    "elif PtO_fun_choice == 2:\n",
    "    PtO_fun = fun2\n",
    "elif PtO_fun_choice == 3:\n",
    "    PtO_fun = fun3\n",
    "elif PtO_fun_choice == 4:\n",
    "    PtO_fun = rosenbrock\n",
    "else:\n",
    "    raise( ValueError('Specify Proper PtO choice!') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Sample from $\\Lambda$\n",
    "_Here we implement uniform random priors on the unit hypercube_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2 # Specify input space dimension (n)\n",
    "num_samples = int(1E3) # number of input samples (N)\n",
    "s_set = sample.sample_set(size=(num_samples, input_dim))\n",
    "\n",
    "if PtO_fun_choice == 1:\n",
    "    s_set.set_dist('normal', {'loc': 0, 'scale': 1})\n",
    "elif PtO_fun_choice == 2:\n",
    "    s_set.set_dist('normal', {'loc': 0, 'scale': 1})\n",
    "elif PtO_fun_choice == 3:\n",
    "    s_set.set_dist('normal', {'loc': 0, 'scale': 1})\n",
    "elif PtO_fun_choice == 4: # rosenbrock\n",
    "    s_set.set_dist('uniform', {'loc': [-1, -1], 'scale': [2, 1]})\n",
    "                   \n",
    "s_set.generate_samples()\n",
    "\n",
    "lam = s_set.samples # create a pointer for ease of reference later with plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(lam), inds = fixed(None), \n",
    "                    N = widgets.IntSlider(value=500, min = 100, max=5000, step=100, continuous_update=False), \n",
    "                    eta_r = fixed(None), space=fixed(0.05), svd=widgets.Checkbox(value=False), color=widgets.Text(value=\"orange\"),\n",
    "                    view_dim_1 = widgets.IntSlider(value=0, min=0, max=input_dim-1, step=1, continuous_update=False), \n",
    "                    view_dim_2 = widgets.IntSlider(value=input_dim-1, min=0, max=input_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Compute Data Space $O(\\Lambda) = \\mathcal{D}$ \n",
    "\n",
    "Format: `(n_dims, n_samples)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_set = sample.map_samples_and_create_problem(s_set, PtO_fun)\n",
    "D = p_set.output.samples\n",
    "\n",
    "# this is how we handle trying to infer the dimension based on what the map put out.\n",
    "try:\n",
    "    output_dim = D.shape[1] # if your function was coded correctly, you should have an (n, d) data space.\n",
    "except IndexError:\n",
    "    print(Warning(\"Warning: Your map might be returning the wrong dimensional data.\"))\n",
    "    try:\n",
    "       output_dim = D.shape[0] \n",
    "    except IndexError:\n",
    "        print(Warning(\"Warning: Guessing it's 1-dimensional.\"))\n",
    "        output_dim = 1\n",
    "print('dimensions :  lambda = '+str(lam.shape)+'   D = '+str(D.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Push-Forward of the Prior $P_{O(\\Lambda)}$\n",
    "_ ... i.e. Characterize the Data Space_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interactive Marginal Visualization\n",
    "p_set.compute_pushforward_dist()\n",
    "pf_dist = p_set.pushforward_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(pf_dist), inds = fixed(None), \n",
    "        N = widgets.IntSlider(value=500, min = 100, max=5000, step=100, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=fixed(False), color=widgets.Text(value=\"brown\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Observed Probability Measure $P_\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PtO_fun_choice == 4:\n",
    "    p_set.set_observed_dist('normal', {'loc': 100, 'scale': 12}) # FOR ROSENBROCK\n",
    "\n",
    "elif PtO_fun_choice == 3:\n",
    "#         p_set.set_observed_dist('normal', {'loc':[0, 0, 0], 'scale':[0.5, 0.25, 1]}) # better for function choice = 2\n",
    "    p_set.set_observed_dist('uniform', {'loc':[-0.5, -0.5, -0.5], 'scale':[1, 1, 1]}) # better for function choice = 2\n",
    "\n",
    "elif PtO_fun_choice == 2:\n",
    "    p_set.set_observed_dist('uniform', {'loc':[0, 0], 'scale':[1, 0.25]}) # default is normal based on the data space # for function choice = 1\n",
    "\n",
    "elif PtO_fun_choice == 1:\n",
    "    p_set.set_observed_dist('uni', {'loc':0, 'scale':0.3}) # default is normal based on the data space # for function choice = 1\n",
    "\n",
    "obs_dist = p_set.observed_dist # this is define a pointer for ease of reference.\n",
    "\n",
    "widgets.interactive(pltdata, data = fixed(obs_dist), inds = fixed(None), \n",
    "        N = widgets.IntSlider(value=500, min = 100, max=5000, step=100, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=fixed(False), color=widgets.Text(value=\"wine\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "At this point we have performed the computations we need to. We have evaluated the input points through our map and performed a KDE on them. It would be useful at this point to save this object and/or its evaluation at every point in the data space for later re-use. Doing so here would be an appropriate place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Accept/Reject Sampling of Posterior\n",
    "\n",
    "Since we have already used the samples in our prior to compute the pushforward density, we can re-use these with an accept/reject algorithm to get a set of samples generated from the posterior according to the solution of the stochastic inverse problem as outlined in the Consistent Bayes formulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_set.set_ratio()\n",
    "eta_r = p_set.ratio\n",
    "solve.problem(p_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_inds = p_set.accept_inds\n",
    "lam_accept = p_set.input.samples[accept_inds,:]\n",
    "num_accept = len(accept_inds)\n",
    "print('Number accepted: %d = %2.2f%%'%(num_accept, 100*np.float(num_accept)/num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Posterior Density\n",
    "### (Visualize Accept/Reject Samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(lam), inds = fixed(accept_inds), \n",
    "        N = widgets.IntSlider(value=num_accept/2, min = 2, max=num_accept+1, step=1, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=widgets.Checkbox(value=False), color=widgets.Text(value=\"orange\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=input_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=input_dim-1, min=0, max=input_dim-1, step=1, continuous_update=False))\n",
    "\n",
    "# You will visualize the accepted samples in a subset of size N of the input samples. \n",
    "# This is mostly for faster plotting, but also so you can see the progression of accepted sampling in the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Now what? \n",
    "\n",
    "Well, we can...\n",
    "\n",
    "## _Visualize the Quality of our SIP Solution by Comparing it to the Observed_\n",
    "_We compare the push-forward of the posterior using accepted samples against the observed density_  \n",
    "_(SIP = Stochastic Inverse Problem)_\n",
    "### Observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(obs_dist), inds = fixed(None), \n",
    "        N = widgets.IntSlider(value=500, min = 100, max=5000, step=100, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=fixed(False), color=widgets.Text(value=\"wine\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushforward of Posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(D), inds = fixed(accept_inds), \n",
    "        N = widgets.IntSlider(value=num_accept/2, min = 2, max=num_accept-1, step=1, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=fixed(False), color=widgets.Text(value=\"eggplant\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Posterior in Other Ways...\n",
    "\n",
    "## Utilizing a KDE of the Posterior\n",
    "### _Use KDE of posterior to generate new samples_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dens_kde = distributions.gkde(lam_accept)\n",
    "n = 5000 # number of new samples to generate using the KDE\n",
    "new_inputs = post_dens_kde.rvs(n)\n",
    "\n",
    "widgets.interactive(pltdata, data = fixed(new_inputs), inds = fixed(None), \n",
    "        N = widgets.IntSlider(value=1000, min = 1, max=5000, step=100, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=widgets.Checkbox(value=False), color=widgets.Text(value=\"tiffany blue\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=input_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=input_dim-1, min=0, max=input_dim-1, step=1, continuous_update=False))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These don't do an excellent job of simulating the accepted samples per say. \n",
    "\n",
    "### _Perhaps these do a decent job resolving the observed density? Let's push-forward the samples from the KDE of the Posterior through our map and see:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outputs = PtO_fun(new_inputs).reshape((n, output_dim))\n",
    "widgets.interactive(pltdata, data = fixed(new_outputs), inds = fixed(None), \n",
    "        N = widgets.IntSlider(value=1000, min = 1, max=5000, step=100, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=widgets.Checkbox(value=False), color=widgets.Text(value=\"cyan\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _More likely, we would use the KDE instead to generate proposal samples for accept/reject._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also outlines the tricks we have to do to handle multi-dimensional parametric distributions. \n",
    "ob = obs_dist.pdf(new_outputs).prod(axis=1).reshape(n)\n",
    "pf = pf_dist.pdf(new_outputs).reshape(n)\n",
    "new_ratios = ob/pf\n",
    "new_inds = solve.perform_accept_reject(new_inputs, new_ratios)\n",
    "new_accepted_samples = new_inputs[new_inds,:]\n",
    "new_num_accept = new_accepted_samples.shape[0]\n",
    "print(\"Num. Accepted = %d out of %d (%2.2f%%)\"%(new_num_accept, n, 100*new_num_accept/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(new_inputs), inds = fixed(new_inds), \n",
    "        N = widgets.IntSlider(value=new_num_accept/2, min = 1, max=new_num_accept, step=1, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=widgets.Checkbox(value=False), color=widgets.Text(value=\"tiffany blue\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=input_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=input_dim-1, min=0, max=input_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Posterior to Construct a Piecewise-Defined Parametric Mixture Distribution\n",
    "\n",
    "_The premise here is to partition the parameter space into affine subspaces (clusters) using the accepted samples. Then, perform singular value decompositions to decompose each subspace into its natural axes. Once this is achieved, we use the projections of the samples in each cluster onto the respective sets of axes in order to fit an parametric distribution. Each one is defined as the product of the one-dimensional marginal distributions that were determined to be the best fit for the projected samples on each axis. Finally, the mixture distribution is weighted according to the proportions of samples in each cluster._\n",
    "\n",
    "This is just a high-dimensional approximation to a density. Likely a poor one. We may use this approach to construct an approximate pushforward disribution by applying the same principles to mapped samples in the data space. _More on that later._\n",
    "\n",
    "_Ex:_ If you have $K$ clusters in an $n$-dimensional parameter space, you have to fit $n\\times K$ parameteric distributions. ($n$ axes for each of the $K$ clusters). The density of any value will then be evaluated as follows:\n",
    "\n",
    "\n",
    "    1. Determine which cluster the sample belongs to.\n",
    "    2. Project the sample onto each orthogonal axis that you got from the SVD\n",
    "    3.  Evaluate the one-dimensional parametric distributions that we found to be the best fit at this projection value.\n",
    "    4.  The product of these two weighted by the proportion of this cluster compared to the total number of samples is then the density value that is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "# import relevant ML modules\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rstate = 42 # random state.\n",
    "sc = StandardScaler()\n",
    "\n",
    "X = lam_accept\n",
    "X_sc = sc.fit_transform(X) # scale data to normalized\n",
    "\n",
    "# train_size = 0.2\n",
    "\n",
    "# X_train, X_test = \\\n",
    "#     train_test_split(X, test_size= 1.0 - train_size, random_state=rstate)\n",
    "\n",
    "# sc = StandardScaler() # instantiate class which keeps track of scaling info.\n",
    "# X_train_sc = sc.fit_transform(X_train)\n",
    "# X_test_sc = sc.transform(X_test)\n",
    "X_train_sc = X_sc # use the entirety of the accepted samples. The downside of this is that you don't \n",
    "\n",
    "h = 0.05 # mesh parameter for plotting\n",
    "buff = 0.1 # buffer for plotting\n",
    "x_min, x_max = X_train_sc[:, 0].min() - buff, X_train_sc[:, 0].max() + buff\n",
    "y_min, y_max = X_train_sc[:, 1].min() - buff, X_train_sc[:, 1].max() + buff\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # mesh params unscaled\n",
    "\n",
    "num_clusters = 8\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=rstate).fit(X_train_sc)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.tree import DecisionTreeClassifier%matplotlib inline\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# kmeans.labels_\n",
    "# kmeans.cluster_centers_\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# score = kmeans.score(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (30, 10)\n",
    "plt.figure()\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.PuRd, alpha=.8)\n",
    "ax.scatter(X_train_sc[:,0], X_train_sc[:,1])\n",
    "# ax.scatter(X_test_sc[:,0], X_test_sc[:,1], c='k', alpha=0.05)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_title('KMeans', fontsize=24)\n",
    "# ax.text(xx.max() - buff, yy.min() + buff, ('%.2f' % score).lstrip('0'), size=25, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_[0])\n",
    "# svd\n",
    "labels = kmeans.labels_\n",
    "# for k in range(num_clusters): # for each cluster k\n",
    "D = { k: {'center': sc.inverse_transform( kmeans.cluster_centers_[k] ), \n",
    "         'samples': lam_accept[labels == k, :]} for k in range(np.unique(labels).size) }\n",
    "\n",
    "# print out what D is holding\n",
    "# num_samps = 2\n",
    "# for k in D.keys():\n",
    "#     print('index:', k, '\\ncenter:', D[k]['center'], \n",
    "#           '\\nfirst few samples:\\n', D[k]['samples'][0:num_samps,:], '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lcl_pca(D):\n",
    "    # the keys of the dictionary are labels for the clusters\n",
    "    dim = len(D[0]['center'])\n",
    "    for k in D.keys():\n",
    "        lam_local = D[k]['samples']\n",
    "        num_local_samples = len(lam_local)\n",
    "        c = D[k]['center']\n",
    "        L = lam_local - c # center the samples at the origin in order to perform SVD\n",
    "        S = np.linalg.svd(L)\n",
    "        B = S[2] # grab the eigenbasis for the subspace\n",
    "        # now we project our samples onto this orthonormal basis (hence no denominator in projection formula)\n",
    "        D[k]['proj_samples'] = {}\n",
    "        for d in range(dim):\n",
    "            Lp =  [np.inner( B[:,d], L[i,:])*B[:,d] for i in range(num_local_samples) ]\n",
    "            Lp = np.array(Lp) + c\n",
    "            D[k]['proj_samples'][d] = Lp # store the projections as a dictionary in a new key\n",
    "#     return D # dictionaries are passed by reference, so no need to return anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Projections on dictionary object that holds our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcl_pca(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "c = D[k]['center']\n",
    "plt.figure()\n",
    "for d in D[k]['proj_samples'].keys(): # for each axis\n",
    "    lam_local = D[k]['samples']\n",
    "    plt.scatter(lam_local[:,0], lam_local[:,1], s=15, alpha=0.3)\n",
    "    lam_local_proj = D[k]['proj_samples'][d] # grab projected samples3\n",
    "    plt.scatter(lam_local_proj[:,0], lam_local_proj[:,1], c='k', s=5, alpha=0.4)\n",
    "    plt.scatter(c[0], c[1], s=200, c='k')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 10)\n",
    "\n",
    "plt.figure()\n",
    "for k in D.keys(): # for each subspace\n",
    "    lam_local = D[k]['samples']\n",
    "    plt.scatter(lam_local[:,0], lam_local[:,1], s=15, alpha=0.3)\n",
    "    c = D[k]['center']\n",
    "    for d in D[k]['proj_samples'].keys(): # for each axis\n",
    "        lam_local_proj = D[k]['proj_samples'][d] # grab projected samples\n",
    "        plt.scatter(lam_local_proj[:,0], lam_local_proj[:,1], c='k', s=5, alpha=0.4)\n",
    "        plt.scatter(c[0], c[1], s=200, c='k')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Parametric Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior_type = 'uniform'\n",
    "obs_type = 'normal'\n",
    "oberved_density = sstats.norm(loc=100., scale=6.)\n",
    "prior_density = sstats.uniform(loc=-1, scale = 2)\n",
    "\n",
    "def fit_dist(D, prior_type, ob_type):\n",
    "    handles = ['pr', 'ob']\n",
    "    for k in D.keys():\n",
    "        D[k]['parametric'] = {d: {dist: {'loc': None, 'scale': None, 'fit': None} for dist in handles}  \n",
    "                              for d in D[k]['proj_samples'].keys() }\n",
    "#         print( D[k]['parametric'] )\n",
    "        for d in D[k]['proj_samples'].keys(): \n",
    "            lcl_proj_samps = D[k]['proj_samples'][d][:,d]\n",
    "            \n",
    "            for idx, dist in enumerate([prior_type, ob_type]): # we have to figure out which axis is associated with distribution\n",
    "                if dist == 'normal':\n",
    "                    lcl_loc = np.mean(lcl_proj_samps)\n",
    "                    lcl_scale = np.std(lcl_proj_samps) # std or var? I think std\n",
    "                    D[k]['parametric'][d][ handles[idx] ]['loc'] = lcl_loc\n",
    "                    D[k]['parametric'][d][ handles[idx] ]['scale'] = lcl_scale\n",
    "                    D[k]['parametric'][d][ handles[idx] ]['fit'] = sstats.kstest(lcl_proj_samps,  sstats.norm(loc=lcl_loc, scale=lcl_scale).cdf)[1]\n",
    "                elif dist == 'uniform':\n",
    "                    lcl_loc = np.min(lcl_proj_samps) # left endpoint\n",
    "                    lcl_scale = np.max(lcl_proj_samps) - lcl_loc # interval length\n",
    "                    D[k]['parametric'][d][ handles[idx] ]['loc'] = lcl_loc\n",
    "                    D[k]['parametric'][d][ handles[idx] ]['scale'] = lcl_scale\n",
    "                    D[k]['parametric'][d][ handles[idx] ]['fit'] = sstats.kstest(lcl_proj_samps,  sstats.uniform(loc=lcl_loc, scale=lcl_scale).cdf)[1]\n",
    "               \n",
    "        # now that we've computed all the test stats for this cluster, we decide which is which\n",
    "        # figure out which dimension fits the oberved best and prior best:\n",
    "        ob_fits = [D[k]['parametric'][d]['ob']['fit'] for d in D[k]['proj_samples'].keys() ]\n",
    "        pr_fits = [D[k]['parametric'][d]['pr']['fit'] for d in D[k]['proj_samples'].keys() ]\n",
    "        \n",
    "        # this loop will select the best fit, but sometimes that means the mixture might end up being ob/ob... which we dont want.\n",
    "#         for d in D[k]['proj_samples'].keys(): \n",
    "#             # if the prior fits better than the ob:\n",
    "#             if D[k]['parametric'][d][ handles[0] ]['fit'] > D[k]['parametric'][d][ handles[1] ]['fit']:\n",
    "#                 D[k]['parametric'][d]['dist'] = {'type': prior_type, \n",
    "#                                                  'loc': D[k]['parametric'][d][ handles[0] ]['loc'],\n",
    "#                                                  'scale': D[k]['parametric'][d][ handles[0] ]['scale']}\n",
    "#             else:\n",
    "#                 D[k]['parametric'][d]['dist'] = {'type': ob_type, \n",
    "#                                                  'loc': D[k]['parametric'][d][ handles[1] ]['loc'],\n",
    "#                                                  'scale': D[k]['parametric'][d][ handles[1] ]['scale']}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "# lcl_proj_samps = D[k]['proj_samples'][d]\n",
    "fit_dist(D, prior_type, obs_type)\n",
    "[D[k]['parametric'][d]['ob']['fit']  for d in D[k]['proj_samples'].keys() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[D[k]['parametric'][d]['pr']['fit']  for d in D[k]['proj_samples'].keys() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, d = 1, 1\n",
    "# lcl_proj_samps = D[k]['proj_samples'][d]\n",
    "fit_dist(D, prior_type, obs_type)\n",
    "D[k]['parametric'][d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "test_py_env",
   "language": "python",
   "name": "test_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
